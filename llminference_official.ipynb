{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee4594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For GOOGLE COLAB and similar platform Users:\n",
    "#### Make sure to select a GPU in the online platform. Don't run this code with a CPU (it will be too slow)\n",
    "\n",
    "# If you are running this code locally, your GPU should be selected automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7c7a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only if you havent installed these libraries already outside of the notebook\n",
    "#!pip install -q ipdb\n",
    "#!pip install -q transformers\n",
    "\n",
    "# And if you are not in Google Colab and you didn't yet install Pytorch, make sure to do it:\n",
    "# find the ideal pytorch installation command at https://pytorch.org/get-started/locally/\n",
    "\n",
    "# Official Notebook #vj30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ff85baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 24 18:02:19 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.59                 Driver Version: 556.13         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   42C    P0             22W /   80W |       0MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# You can use this command to view information about your GPU and the amount of free memory it has\n",
    "# Make sure that you have at last 4GB of free GPU memory to do this course\n",
    "!nvidia-smi \n",
    "# If you are using Google Colab or a similar online platform, make sure to select a GPU in the menus\n",
    "# In Google colab, at the moment the option is within the Runtime menus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad59190e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you seem to have already downloaded the files. If you wish to re-download them, delete the llm.py file\n"
     ]
    }
   ],
   "source": [
    "# If you are running this online (for example at Google Colab), \n",
    "# make sure you have the support files on the same folder\n",
    "# Otherwise run this cell to download them\n",
    "\n",
    "# NOTE: Downloading will take a while, be patient. You can refresh your folder from time to time to see when the files\n",
    "# have been created.\n",
    "\n",
    "import os, requests, zipfile, io \n",
    "\n",
    "files_url = \"https://ideami.com/llm_align\"\n",
    "\n",
    "# Downloading proceeds if we detect that one of the key files to download is not present\n",
    "if not os.path.exists(f\"llm.py\"):\n",
    "    print(\"Downloading files using Python\")\n",
    "    response = requests.get(files_url)\n",
    "    zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")\n",
    "else:\n",
    "    print(\"you seem to have already downloaded the files. If you wish to re-download them, delete the llm.py file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d8ca543-b30c-49d0-95ff-166bedd5d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Pytorch\n",
    "import torch\n",
    "# Architecture\n",
    "import transformers\n",
    "# Import Llama based model\n",
    "from llm import Llama, ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "727a9850-348a-4bec-95b1-48fc61f3d749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode::Using pretrained model without alignment\n",
      "Using model base_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5857/1877504702.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(os.path.join(model_path, model_inf), map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 138.43 M parameters\n"
     ]
    }
   ],
   "source": [
    "use_orpo = False  # use aligned checkpoint or not\n",
    "num_answers = 3\n",
    "temp = 1\n",
    "topk= 50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "tokenizer_path = \"tokenizers/tok16384\"\n",
    "model_path = \"./models/\"\n",
    "    \n",
    "if use_orpo==True:\n",
    "        model_inf, context= \"aligned_model.pt\", 1024  # ORPO is trained with context of 1024\n",
    "        print(\"Mode::Using Orpo aligned model\")\n",
    "else:\n",
    "        model_inf, context= \"base_model.pt\", 512  # The original was trained with context of 512\n",
    "        print(\"Mode::Using pretrained model without alignment\")\n",
    "\n",
    "print(f\"Using model {model_inf}\")\n",
    "   \n",
    "# Load model and extract config\n",
    "checkpoint = torch.load(os.path.join(model_path, model_inf), map_location=device)\n",
    "config = checkpoint.pop(\"config\")\n",
    "    \n",
    "# temporary fix if the model was trained and saved with torch.compile\n",
    "# The _orig_mod. prefix in your model's state dictionary keys is related to\n",
    "# how PyTorch handles compiled models, specifically when using the torch.compile function\n",
    "# When torch.compile is used, PyTorch might wrap the original model in a way that modifies\n",
    "# the names of its parameters and buffers. This wrapping can prepend a prefix like _orig_mod.\n",
    "# We remove those wrappings to make the checkpoint compatible with the non compiled version of the model\n",
    "new_dict = dict()\n",
    "for k in checkpoint.keys():\n",
    "        if k.startswith(\"_orig_mod.\"):\n",
    "            #print(\"Removing _orig_mod wrapping\")\n",
    "            new_dict[k.replace(\"_orig_mod.\", \"\")] = checkpoint[k]\n",
    "        else:\n",
    "            new_dict[k] = checkpoint[k]\n",
    "\n",
    "# Setup tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_args = ModelArgs(\n",
    "        dim=config.hidden_size, \n",
    "        n_layers=config.num_hidden_layers, \n",
    "        n_heads=config.num_attention_heads, \n",
    "        n_kv_heads=config.num_key_value_heads, \n",
    "        vocab_size=config.vocab_size, \n",
    "        norm_eps=config.rms_norm_eps, \n",
    "        rope_theta=config.rope_theta,\n",
    "        max_seq_len=context, \n",
    "        dropout=config.attention_dropout, \n",
    "        hidden_dim=config.intermediate_size,\n",
    "        attention_bias=config.attention_bias,\n",
    "        mlp_bias=config.mlp_bias\n",
    "    )\n",
    "\n",
    "# Instantiate model, load parms, move to device\n",
    "model = Llama(model_args)\n",
    "model.load_state_dict(new_dict)\n",
    "if device.type == 'cuda':\n",
    "        model = model.to(torch.bfloat16)\n",
    "        model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"Model size: {model_size/1e6:.2f} M parameters\")\n",
    "\n",
    "# Interactive loop\n",
    "while True:\n",
    "         qs = input(\"Enter text (q to quit) >>> \")\n",
    "         if qs == \"\":\n",
    "             continue\n",
    "         if qs == 'q':\n",
    "             break\n",
    "  \n",
    "         # we activate chat template only for ORPO model because it was trained with it\n",
    "         if use_orpo:\n",
    "            qs = f\"<s> <|user|>\\n{qs}</s>\\n<s> <|assistant|> \"\n",
    "\n",
    "         x = tokenizer.encode(qs)\n",
    "         x = torch.tensor(x, dtype=torch.long, device=device)[None, ...]\n",
    "\n",
    "         for ans in range(num_answers):\n",
    "            with torch.no_grad():\n",
    "                y = model.generate(\n",
    "                    x, \n",
    "                    max_new_tokens=256, \n",
    "                    temperature=temp, \n",
    "                    top_k=topk\n",
    "                )\n",
    "\n",
    "            response = tokenizer.decode(y[0].tolist(), skip_special_tokens=True)   \n",
    "\n",
    "            output = model.clean_response(response)\n",
    "\n",
    "            print(\"################## \\n\")\n",
    "            print(f\"### Answer {ans+1}: {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b597da-a0c9-418d-89da-97be7da9c19a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
