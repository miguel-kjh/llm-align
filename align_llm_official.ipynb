{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd51f6-e9df-4ae0-91a8-9fe80040ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Alignment with ORPO Technique - Notebook created by Javier Ideami (ideami.com)\n",
    "# ORPO: Monolithic Preference Optimization without Reference Model (https://arxiv.org/abs/2403.07691)\n",
    "\n",
    "# LLM Alignment needs to be applied to an LLM that is a bit more powerful than the basic one we trained earlier\n",
    "# That's why I am providing the checkpoint of an already trained more powerful LLM (138 million parameters vs 19 million of ours)\n",
    "# so that we can apply ORPO alignment on top of it.\n",
    "# The pretrained 138 million parameter model has been pre-trained on the open source Fineweb-Edu dataset.\n",
    "\n",
    "# Traditional alignment techniques like RLHF take a long time and money to be implemented. New techniques like ORPO simplify alignment a lot.\n",
    "# We will be able to train ORPO alignment with 1 single GPU and little GPU memory (around 4 gigabytes)  \n",
    "# Of course results will be imperfect because we are still applying ORPO on top of a model that is small and has been trained with much\n",
    "# less data than the large models out there, and for a much shorter amount of time. But it will be enough to compare the before and after,\n",
    "# and to see how ORPO improves the way the LLM communicates in many occasions.\n",
    "# And we will understand every part of the code that makes it possible.\n",
    "\n",
    "# This file includes code from the open source ORPO repository licensed under the Apache License 2.0.\n",
    "# See licenses/orpo-license for details.\n",
    "# Modifications: Variable names have been changed for educational purposes.\n",
    "\n",
    "# This file also uses the ORPO-DPO-mix-40k dataset licensed under the Apache License 2.0.\n",
    "# See licenses/orpo-dpo-mix-40k-license for details.\n",
    "\n",
    "# Official notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baa52df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For GOOGLE COLAB and similar platform Users:\n",
    "#### Make sure to select a GPU in the online platform. Don't run this code with a CPU (it will be too slow)\n",
    "\n",
    "# If you are running this code locally, your GPU should be selected automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678006f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run the following installation lines ONLY if you havent installed these libraries already outside of the notebook\n",
    "#!pip install -q ipdb\n",
    "#!pip install -q tqdm\n",
    "#!pip install -q datasets\n",
    "#!pip install -q transformers\n",
    "#!pip install -q wandb\n",
    "\n",
    "# And if you are not in Google Colab and you didn't yet install Pytorch, make sure to do it:\n",
    "# find the ideal pytorch installation command at https://pytorch.org/get-started/locally/\n",
    "\n",
    "# in addition, you may try to install the flash-attn library, although at the moment it is having issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1308c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this command to view information about your GPU and the amount of free memory it has\n",
    "# Make sure that you have at last 4GB of free GPU memory to do this course\n",
    "!nvidia-smi \n",
    "# If you are using Google Colab or a similar online platform, make sure to select a GPU in the menus\n",
    "# In Google colab, at the moment the option is within the Runtime menus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ab151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary files and create necessary folders\n",
    "# llm.py - llm model: an llm architecture that is more powerful\n",
    "# models folder: pretrained checkpoints for the base and the aligned model\n",
    "# data folder: tokenized dataset stored in this folder\n",
    "# tokenizers folder: pretrained tokenizer on the large dataset FineWeb-Edu\n",
    "\n",
    "# NOTE: Downloading will take a while, be patient. You can refresh your folder from time to time to see when the files\n",
    "# have been created. If you have any problems downloading the files with this code, I have also added llm_align.zip\n",
    "# to the downloadable resources of this lecture (however, best option is to use this code, because then you don't need\n",
    "# to upload the files or do anything else)\n",
    "\n",
    "import os, requests, zipfile, io \n",
    "\n",
    "files_url = \"https://ideami.com/llm_align\"\n",
    "\n",
    "# Downloading proceeds if we detect that one of the key files to download is not present\n",
    "if not os.path.exists(f\"llm.py\"):\n",
    "    print(\"Downloading files using Python\")\n",
    "    response = requests.get(files_url)\n",
    "    zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")\n",
    "else:\n",
    "    print(\"you seem to have already downloaded the files. If you wish to re-download them, delete the llm.py file\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9db2545-dc98-4d71-b259-78ee59dfa753",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import necessary libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import ipdb\n",
    "\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Hugging Face libraries that will accelerate our implementation\n",
    "import transformers\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "\n",
    "# These lines improve performance for Ampere Architecture (e.g: A100s)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "# Empty GPU cache memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Optional, for debugging, if you want to view entire tensors and values in a more comfortable way, etc\n",
    "torch.set_printoptions(threshold=10000)\n",
    "torch.set_printoptions(sci_mode=False, precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9054f1b2-d06f-4165-8d71-75feac4b2acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING PARAMETERS\n",
    "batch_size = 1  # you can change it to 2 if you have enough GPU memory \n",
    "epochs = 3\n",
    "lr = 6e-5 \n",
    "lr_warmup_steps = 100 # learning rate warmup phase\n",
    "context = 1024\n",
    "alpha = 0.5 # weighting for the ORPO odds ration\n",
    "prompt_max_size = 512 # limit for the prompt part of the interaction\n",
    "compile = False # Compile improves performance in compatible systems\n",
    "dtype = torch.bfloat16 # Setting precision for calculations\n",
    "log_iters = 100\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "dropout = 0.\n",
    "grad_clip = 1.0\n",
    "weight_decay = 0.0\n",
    "\n",
    "# DEVICE - Set device to GPU or CPU (use GPU definitely)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device: You will be using: \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810053b-5e06-49ef-8a7c-2b4988f6b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGGING\n",
    "project_name=\"test\"\n",
    "wandb_log = True\n",
    "wandb_project = project_name\n",
    "wandb_run_name = \"test-run\" + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)\n",
    "\n",
    "# The first time you run this logging code set to True, the weights and biases library\n",
    "# will ask you for an API key. You can follow the instructions in the video, or you can\n",
    "# also simply click on a link that should appear when you run this cell, pointing to this\n",
    "# address: https://wandb.ai/authorize  \n",
    "# Going to that address will allow you to quickly get an API key as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77f66f-e3c7-4ea0-ba33-fc8792827ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET paths\n",
    "dataset_path = \"./data/orpo_dataset\" # where the orpo dataset will be stored\n",
    "\n",
    "# This is a special dataset prepared for ORPO alignment training. It is available at HuggingFace\n",
    "# https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k/blob/main/README.md\n",
    "dataset_name = \"mlabonne/orpo-dpo-mix-40k\" # path to the huggingface orpo dataset\n",
    "\n",
    "tokenizer_path = \"tokenizers/tok16384\" # path to the tokenizer\n",
    "checkpoint_dir = './models/'  # where we store checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a8b95-0b03-4e26-ae35-0e9d6a82f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the Dataset\n",
    "###########\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_path) # Load tokenizer in HuggingFace format\n",
    "\n",
    "# Set our interaction template\n",
    "tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "\n",
    "# Make padding token equal to the end of sentence token (which has id of 2 in this case)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# If you want to debug the tokenization of the dataset, delete the orpo_dataset folder or change\n",
    "# this dataset_path to something else like \"./data/tmp\"\n",
    "# uncomment to activate the preprocessing of dataset\n",
    "#dataset_path = \"./data/tmp\" # delete this folder to preprocess again\n",
    "\n",
    "# If dataset has already been processed and tokenized, we can load it directly from our disk\n",
    "if os.path.exists(dataset_path):\n",
    "    print(\"Loading encoded dataset from disk\")\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "# Otherwise, we will load the dataset from huggingface and then filter it and tokenize it and save it\n",
    "else:\n",
    "    print(\"Preprocessing and tokenizing dataset\")\n",
    "    dataset = load_dataset(dataset_name, split=\"all\")\n",
    "\n",
    "    # Optional: Filter Toxic entries / without vs with this filter: 37136 vs 36622 elements aprox\n",
    "    dataset = dataset.filter(lambda r: r[\"source\"] != \"toxic-dpo-v0.2\")\n",
    "\n",
    "    # FILTER DATASET\n",
    "    # This function will eliminate entries that are longer than 512(prompt_max_size). This is important because we want prompt+answer to fit\n",
    "    # within the total context (1024).\n",
    "    def filter_dataset(examples):\n",
    "        # examples['chosen'][:-1] picks the prompt minus the answer\n",
    "        prompt_length = tokenizer.apply_chat_template(examples['chosen'][:-1], tokenize=True, add_generation_prompt=True, return_tensors='pt').size(-1)  \n",
    "        # Preserve only samples that have a prompt smaller than prompt_max_size\n",
    "        if prompt_length < prompt_max_size:    \n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    # PREPROCESS DATASET: tokenize it and store fields you will need later    \n",
    "    # HF Tokenizer Dict Format\n",
    "    # Encoding(num_tokens=1024, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
    "    def preprocess_dataset(examples: Union[List, Dict]):\n",
    "        # processed in batches of 1000 by default\n",
    "        # Take chosen field, eliminate last answer, apply template adding assistant prompt - explore: prompt[0]\n",
    "        prompt = [tokenizer.apply_chat_template(item[:-1], tokenize=False, add_generation_prompt=True) for item in examples['chosen']]\n",
    "        # very important: some of the samples are multi-turn conversations. The prompt includes all interactions between user and assistant\n",
    "        # until the last question. We remove the last answer and all the previous interaction becomes the prompt.\n",
    "\n",
    "        # Take the chosen field, then apply chat template \n",
    "        chosen = [tokenizer.apply_chat_template(item, tokenize=False) for item in examples['chosen']]\n",
    "\n",
    "        # Take the rejected field, then apply chat template \n",
    "        rejected = [tokenizer.apply_chat_template(item, tokenize=False) for item in examples['rejected']]\n",
    "        \n",
    "        # Tokenice the prompt\n",
    "        inputs = tokenizer(prompt,   max_length=context, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        # model_inputs will have dict fields input_ids and attention_mask (e.g: model_inputs.input_ids[0])\n",
    "        # testing by decoding: tokenizer.decode(model_inputs.input_ids[0])\n",
    "\n",
    "        # Important, all elements will have same length of 1024 tokens, extra padding tokens will be added to reach 1024\n",
    "\n",
    "        # Tokenice the chosen positive response\n",
    "        pos_labels   = tokenizer(chosen,   max_length=context, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        # pos_labels will have dict fields input_ids and attention_mask (e.g: pos_labels.input_ids[0])\n",
    "        # testing by decoding: tokenizer.decode(pos_labels.input_ids[0])\n",
    "        \n",
    "        # Tokenice the rejected negative response\n",
    "        neg_labels   = tokenizer(rejected, max_length=context, padding='max_length', truncation=True, return_tensors='pt') \n",
    "        # same as before\n",
    "\n",
    "        # Add the chosen-positive and rejected-negative response ids and masks to the prompt ones, so that we have it all in -inputs-\n",
    "        inputs['positive_input_ids'] = pos_labels['input_ids']\n",
    "        inputs['positive_attention_mask'] = pos_labels['attention_mask']\n",
    "\n",
    "        inputs['negative_input_ids'] = neg_labels['input_ids']\n",
    "        inputs['negative_attention_mask'] = neg_labels['attention_mask']\n",
    "\n",
    "        return inputs\n",
    "    \n",
    "    # Filter dataset to exclude prompts that are too long\n",
    "    dataset = dataset.filter(filter_dataset)\n",
    "\n",
    "    # Preprocess the dataset, if you have issues with multiprocessing, make sure to use num_proc=1\n",
    "    # multiprocessing alternative: dataset = dataset.map(preprocess_dataset, batched=True, num_proc=min(32,os.cpu_count()), remove_columns=dataset.column_names)  \n",
    "    dataset = dataset.map(preprocess_dataset, batched=True, num_proc=1, remove_columns=dataset.column_names) \n",
    "    # sent in batches of 1000 by default \n",
    "\n",
    "    # As a result of the preprocessing, dataset variable will have all these internal fields:\n",
    "    #dataset: Dataset({features: ['input_ids', 'token_type_ids', 'attention_mask', 'positive_input_ids', 'positive_attention_mask', 'negative_input_ids', 'negative_attention_mask'],num_rows: 39091})\n",
    "\n",
    "    dataset.save_to_disk(dataset_path)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d7d0b-33a3-45b5-a728-59c19c815c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, you can test some of the content of the dataset with for example these: \n",
    "# dataset[0]['input_ids']  /  dataset[0]['positive_input_ids']\n",
    "# testing Ids to Text: tokenizer.decode(dataset[0]['positive_input_ids'])\n",
    "\n",
    "# Split the data into train and validation, 5% for the validation set\n",
    "dataset = dataset.shuffle(42).train_test_split(test_size=0.05)  \n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "#Dataset({features: ['input_ids', 'token_type_ids', 'attention_mask', 'positive_input_ids', 'positive_attention_mask', 'negative_input_ids', 'negative_attention_mask'],num_rows: 37136})\n",
    "\n",
    "val_data = dataset[\"test\"]\n",
    "#Dataset({features: ['input_ids', 'token_type_ids', 'attention_mask', 'positive_input_ids', 'positive_attention_mask', 'negative_input_ids', 'negative_attention_mask'],num_rows: 1955})\n",
    "\n",
    "# Data_collator efficiently prepares your training and validation data for language modeling by batching, padding (optional), and performing masking (optional) according to your configuration\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Setup DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=False, collate_fn=data_collator, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=data_collator, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069d7400-41b2-474a-b31b-50538f038b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Test your DataLoaders\n",
    "#batch = next(iter(train_loader))\n",
    "#print(tokenizer.decode(batch['input_ids'][0]))  # this will show just the prompt, followed by padding tokens\n",
    "\n",
    "# debug: batch['input_ids'] (shape is 1,1024)  \n",
    "# debug: Ids to Text -> tokenizer.decode(batch['input_ids'][0]) - [0] because needs to address inside batch number dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f9a838-ab30-49f3-9d70-135f55e2bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "###############################################################\n",
    "################### SETUP MODEL ###############################\n",
    "###############################################################\n",
    "###############################################################\n",
    "\n",
    "# Import Llama based model\n",
    "from llm import Llama, ModelArgs\n",
    "\n",
    "# Load Pretrained Model of 138 million parameters\n",
    "checkpoint = torch.load(os.path.join(checkpoint_dir, \"base_model.pt\"))\n",
    "\n",
    "# Extract config from the pretrained model\n",
    "config = checkpoint.pop(\"config\")\n",
    "\n",
    "# Instantiate ModelArgs with the necessary parameters\n",
    "model_args = ModelArgs(\n",
    "    dim=config.hidden_size, \n",
    "    n_layers=config.num_hidden_layers, \n",
    "    n_heads=config.num_attention_heads, \n",
    "    n_kv_heads=config.num_key_value_heads, \n",
    "    vocab_size=config.vocab_size, \n",
    "    norm_eps=config.rms_norm_eps, \n",
    "    rope_theta=config.rope_theta,\n",
    "    max_seq_len=context, \n",
    "    dropout=config.attention_dropout, \n",
    "    hidden_dim=config.intermediate_size,\n",
    "    attention_bias=config.attention_bias,\n",
    "    mlp_bias=config.mlp_bias\n",
    ")\n",
    "\n",
    "#ModelArgs(dim=768, n_layers=12, n_heads=12, n_kv_heads=12, vocab_size=16384, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-06, rope_theta=10000.0, max_seq_len=1024, dropout=0.0, hidden_dim=3072, attention_bias=False, mlp_bias=False)\n",
    "\n",
    "model = Llama(model_args)  # Instantiate model\n",
    "model.load_state_dict(checkpoint)  # Load checkpoint\n",
    "\n",
    "model = model.to(dtype) # Set the precision type\n",
    "model = model.to(device) # Move it to the right device\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Torch.compile compiles a PyTorch model to an optimized version, aiming to improve runtime performance and efficiency.\n",
    "# Disable if your system doesn't support it\n",
    "if compile:\n",
    "    print(\"[INFO] Compiling model\")\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# Print the number of parameters of the model\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \" Million parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c99e1e1-38b5-4e95-a8e4-bfb1ac4e49f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "########## SETUP TRAINING AND OPTIMIZER ###############\n",
    "#######################################################\n",
    "\n",
    "\n",
    "# Declare optimizer, it helps us compute gradients, update parameters, manage learning rate, apply weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95), eps=1e-8, fused= device == 'cuda', weight_decay=weight_decay)\n",
    "# betas: control the exponential moving averages of the gradient and its square (essential part of AdamW alg) \n",
    "# eps: a small number to add numerical stability in computations\n",
    "# fused: technique used to improve the performance of computations, by combining multiple operations into a single one \n",
    "\n",
    "# Calculate max total number of steps, the length of training loader times number of epochs\n",
    "num_training_steps = len(train_loader) * epochs  #111408 with default settings - we use BS of 1 by default\n",
    "\n",
    "# Scheduler for learning rate: first 100 steps we do a warmup in which we increase linearly the LR.\n",
    "# After warmup, we decrease it gradually following a cosine curve\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < lr_warmup_steps:\n",
    "        return float(current_step) / float(max(1, lr_warmup_steps))\n",
    "    progress = float(current_step - lr_warmup_steps) / float(max(1, num_training_steps - lr_warmup_steps))\n",
    "    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(0.5) * 2.0 * progress)))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91bbdb8-4f91-4aca-b5f8-7281243c1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log probabilities for positive and negative responses, necessary for Log Odds Calculation\n",
    "def compute_logps(prompt_attention_mask, chosen_inputs, chosen_attention_mask, logits):\n",
    "\n",
    "    # note: in general we get rid of first element in labels because we want to match each token to the label in next position so \n",
    "    # we shift labels one to the left. As a consequence we get rid of last element of logits to equalize dimensions and also\n",
    "    # because we dont care about the predictions for last token as there is no next token after that\n",
    "\n",
    "    # create mask with only positions of last answer but starting from the character before the last answer,\n",
    "    # because we will start predicting from that one\n",
    "    mask = chosen_attention_mask[:, :-1] - prompt_attention_mask[:, 1:]\n",
    "\n",
    "    # Gather logits corresponding to the IDs of the tokens of chosen answer\n",
    "    # torch.gather selects elements of logits based on the indices in index_tensor along the specified dimension dim=2.\n",
    "    # for example index gives us token 1160. Now we go to logits and from dimension 2 we extract the probability of token 1160\n",
    "    # IMPORTANT: log_softmax function already incorporates the negative sign inside, so it produces negative log probabilities\n",
    "    # logits[:,:-1,:] (1,1023,16384)\n",
    "    # index = (mask * chosen_inputs[:, 1:]).unsqueeze(2)  (1, 1023, 1)\n",
    "    # final result: per_token_logps: 1,1023\n",
    "    per_token_logps = torch.gather(logits[:, :-1, :].log_softmax(-1), dim=2, \n",
    "                                    index=(mask * chosen_inputs[:, 1:]).unsqueeze(2)).squeeze(2)\n",
    "\n",
    "    # mask the per_token_logps to leave only positions of last answer, then normalize\n",
    "    # mask.sum will only sum the active elements of the mask so that you normalize by the total tokens of answer\n",
    "    return torch.mul(per_token_logps, mask.to(dtype)).sum(dim=1).to(dtype) / mask.sum(dim=1).to(dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df56da9-4ef8-42c4-aa0e-0ac4f35c9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def compute_logps(self, prompt_attention_mask, chosen_inputs, chosen_attention_mask, logits):\n",
    "        #mask = chosen_attention_mask[:, :-1] - prompt_attention_mask[:, 1:]\n",
    "        #per_token_logps = torch.gather(logits[:, :-1, :].log_softmax(-1), dim=2, \n",
    "        #                               index=(mask * chosen_inputs[:, 1:]).unsqueeze(2)).squeeze(2)\n",
    "        #return torch.mul(per_token_logps, mask.to(dtype=torch.bfloat16)).sum(dim=1).to(dtype=torch.float64) / mask.sum(dim=1).to(dtype=torch.float64)\n",
    "\n",
    "# this is the code of the toy example we create in the videos in order\n",
    "# to understand in depth how the compute_logps function works\n",
    "# if you wish to run it, uncomment it (you need to uncomment the three quotes at the beginning and also at the end of the code)\n",
    "\n",
    "'''\n",
    "p_mask = torch.tensor([1,1,1,1,0,0,0,0,0,0,0])\n",
    "c_inputs = torch.tensor([2,4,3,2,4,2,1,0,0,0,0])\n",
    "c_mask = torch.tensor([1,1,1,1,1,1,1,0,0,0,0])\n",
    "print(f\"p_mask: {p_mask}\")\n",
    "print(f\"c_inputs: {c_inputs}\")\n",
    "print(f\"c_mask: {c_mask}\")\n",
    "mask = c_mask[:-1] - p_mask[1:] \n",
    "print(f\"mask: {mask}\")\n",
    "logits = torch.tensor([\n",
    "    [0.2, 0.4, 0.8, 0.1, 0.3],\n",
    "    [0.2, 0.1, 0.5, 0.12, 0.31],\n",
    "    [0.22, 0.44, 0.81, 0.13, 0.32],\n",
    "    [0.29, 0.42, 0.84, 0.15, 0.32],\n",
    "    [0.24, 0.48, 0.88, 0.17, 0.34],\n",
    "    [0.21, 0.41, 0.81, 0.14, 0.33],\n",
    "    [0.23, 0.43, 0.82, 0.16, 0.35],\n",
    "    [0.2, 0.4, 0.8, 0.1, 0.3],\n",
    "    [0.2, 0.1, 0.5, 0.12, 0.31],\n",
    "    [0.22, 0.44, 0.81, 0.13, 0.32],\n",
    "    [0.22, 0.44, 0.81, 0.13, 0.32]\n",
    "])\n",
    "\n",
    "print(f\"c_inputs[1:]: {c_inputs[1:]}\")\n",
    "index=(mask * c_inputs[1:])\n",
    "print(f\"index: {index}\")\n",
    "\n",
    "# Expand dimensions for correct gather shape\n",
    "index_expanded = index.unsqueeze(1)\n",
    "print(f\"index_expanded: {index_expanded}\")\n",
    "\n",
    "print(\"shapes: \",index_expanded.shape, logits.shape)\n",
    "# Gather the values at the specified indices\n",
    "gathered_values = torch.gather(logits[:-1,:], dim=1, index=index_expanded)\n",
    "print(f\"gathered: {gathered_values}\")\n",
    "\n",
    "# Squeeze to remove the unnecessary dimension\n",
    "per_token_logps = gathered_values.squeeze(1)\n",
    "print(f\"per_token_logps: {per_token_logps}\")\n",
    "\n",
    "result = torch.mul(per_token_logps, mask)\n",
    "print(f\"result: {result}\")\n",
    "f1 = result.sum(dim=0)\n",
    "f2 = mask.sum(dim=0)\n",
    "print(f\"f1: {f1}\")\n",
    "print(f\"f2: {f2}\")\n",
    "final = f1 / f2\n",
    "print(f\"final: {final}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f773c143-3f45-4aea-9189-39f91186adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Iterators and update key variables\n",
    "val_iterator = iter(val_loader)\n",
    "train_iterator = iter(train_loader)\n",
    "log_iters = 100\n",
    "eval_iters= 5 # Use a small number, otherwise things will get too slow\n",
    "\n",
    "print(f\"train loader size: {len(train_loader)}\")\n",
    "print(f\"validation loader size: {len(val_loader)}\")\n",
    "print(f\"number of training steps: {num_training_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fbf368-73ef-4c1c-aaca-cfa04226e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # Prevent gradient calculation\n",
    "# Calculate average of training and validation losses over multiple batches\n",
    "def calculate_loss():\n",
    "    global train_iterator, val_iterator\n",
    "    loss_mean={}\n",
    "    odds_mean={}\n",
    "    ratio_mean={}\n",
    "    model.eval()\n",
    "    for split in ['train','val']: \n",
    "        l=torch.zeros(eval_iters)  # Create a tensor of zeros the size of eval_iters\n",
    "        o=torch.zeros(eval_iters)  # Create a tensor of zeros the size of eval_iters\n",
    "        r=torch.zeros(eval_iters)  # Create a tensor of zeros the size of eval_iters\n",
    "        for i in range(eval_iters):\n",
    "            try:\n",
    "                if split == 'val':\n",
    "                    batch = next(val_iterator)\n",
    "                else:\n",
    "                    batch = next(train_iterator)\n",
    "            except StopIteration:\n",
    "                if split == 'val':\n",
    "                    print(\"####### Resetting Validation Iterator\")\n",
    "                    val_iterator = iter(val_loader)\n",
    "                    batch = next(val_iterator)\n",
    "                else:\n",
    "                    print(\"####### Resetting Training Iterator\")\n",
    "                    train_iterator = iter(train_loader)\n",
    "                    batch = next(train_iterator)                   \n",
    "\n",
    "            batch[\"positive_input_ids\"] = batch[\"positive_input_ids\"].to(device) \n",
    "            batch[\"positive_attention_mask\"] = batch[\"positive_attention_mask\"].to(device)\n",
    "            batch[\"negative_input_ids\"] = batch[\"negative_input_ids\"].to(device)\n",
    "            batch[\"negative_attention_mask\"] = batch[\"negative_attention_mask\"].to(device)\n",
    "            batch[\"attention_mask\"] = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "            neg_labels = batch['negative_input_ids'].clone()\n",
    "            pos_labels = batch['positive_input_ids'].clone()\n",
    "        \n",
    "            mask = batch['attention_mask'] * batch['positive_attention_mask']  # sets mask to have 1s in only the prompt positions\n",
    "            pos_labels = pos_labels * mask.logical_not()  # puts 0s where the prompt was, preserve last answer (padding tokens are EOS(2))\n",
    "        \n",
    "            pos_labels[pos_labels == 0] = tokenizer.pad_token_id # replaces 0s with EOS(2)\n",
    "            neg_labels[neg_labels == tokenizer.pad_token_id] = -100 # change 2 to -100 so that loss calculations ignore prompt and padding\n",
    "            pos_labels[pos_labels == tokenizer.pad_token_id] = -100 # change 2 to -100 so that loss calculations ignore prompt and padding\n",
    "        \n",
    "            outputs_pos, loss_pos = model(batch['positive_input_ids'], pos_labels)  #  (1,1024) , (1,1024)\n",
    "            outputs_neg, loss_neg = model(batch['negative_input_ids'], neg_labels)    \n",
    "        \n",
    "            # returns the average of the log probabilities for the positive samples (masking out prompt)\n",
    "            pos_prob = compute_logps(\n",
    "                        prompt_attention_mask=batch['attention_mask'], \n",
    "                        chosen_inputs=batch[\"positive_input_ids\"], \n",
    "                        chosen_attention_mask=batch['positive_attention_mask'], \n",
    "                        logits=outputs_pos\n",
    "                    )\n",
    "            # returns the average of the log probabilities for the negative samples (masking out prompt)\n",
    "            neg_prob = compute_logps(\n",
    "                        prompt_attention_mask=batch['attention_mask'], \n",
    "                        chosen_inputs=batch[\"negative_input_ids\"], \n",
    "                        chosen_attention_mask=batch['negative_attention_mask'], \n",
    "                        logits=outputs_neg\n",
    "                    )    \n",
    "        \n",
    "            # CALCULATE ORPO ODDS RATIO\n",
    "            log_odds = (pos_prob - neg_prob) - (torch.log(1 - torch.exp(pos_prob)) - torch.log(1 - torch.exp(neg_prob)))\n",
    "            sig_ratio = F.sigmoid(log_odds) # constrain to be between 0 and 1\n",
    "            ratio = torch.log(sig_ratio) # apply the final log to the calculation\n",
    "        \n",
    "            # Calculate the Final Total Loss, combination of standard Cross Entropy loss and the weighted Odds Ratio\n",
    "            loss = torch.mean(loss_pos - (alpha*ratio).mean()).to(dtype=dtype)\n",
    "            # notice that mean() is useful if batch size is larger than 1  \n",
    "\n",
    "            l[i]=loss.item()\n",
    "            o[i]=log_odds.mean().item()\n",
    "            r[i]=ratio.mean().item()\n",
    "        \n",
    "        loss_mean[split]=l.mean().item()\n",
    "        odds_mean[split]=o.mean().item()\n",
    "        ratio_mean[split]=r.mean().item()\n",
    "        \n",
    "            \n",
    "    model.train()\n",
    "    return loss_mean, odds_mean, ratio_mean\n",
    "\n",
    "l, o, r = calculate_loss()\n",
    "print(l,o,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c3dd71-0ede-4f74-94d2-8f3be0ee40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "################################################\n",
    "############### ORPO TRAINING ##################\n",
    "################################################\n",
    "################################################\n",
    "\n",
    "try:\n",
    "    for e in range(epochs):\n",
    "        for i, batch in tqdm(enumerate(train_loader), total=len(train_loader), dynamic_ncols=True):\n",
    "        \n",
    "            optimizer.zero_grad(set_to_none=True)  # Reset gradients\n",
    "    \n",
    "            # Move batch data to device\n",
    "            batch[\"positive_input_ids\"] = batch[\"positive_input_ids\"].to(device) \n",
    "            batch[\"positive_attention_mask\"] = batch[\"positive_attention_mask\"].to(device)\n",
    "            batch[\"negative_input_ids\"] = batch[\"negative_input_ids\"].to(device)\n",
    "            batch[\"negative_attention_mask\"] = batch[\"negative_attention_mask\"].to(device)\n",
    "            batch[\"attention_mask\"] = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            # Debug: if anytime you want to look inside batch: #tokenizer.decode(batch[\"positive_input_ids\"][0])\n",
    "\n",
    "            # Get the token IDs of positive and negative responses\n",
    "            neg_labels = batch['negative_input_ids'].clone()\n",
    "            pos_labels = batch['positive_input_ids'].clone()\n",
    "    \n",
    "            # CALCULATE STANDARD CROSS ENTROPY LOSS (focused on POSITIVE responses)\n",
    "\n",
    "            # disabling loss on prompt tokens\n",
    "            # When we calculate the standard loss, we will focus on the loss of the positive responses, how good is the model\n",
    "            # predicting the next character in the case of the positive, chosen responses. So we want to mask the positive IDs\n",
    "            # so that they only take into account the ones of the response, and ignore the prompt\n",
    "            mask = batch['attention_mask'] * batch['positive_attention_mask']  # sets mask to have 1s in only the prompt positions\n",
    "            # in our case the line above is similar to just mask = batch['attention_mask'] (because all our batch sequences have same length)\n",
    "            pos_labels = pos_labels * mask.logical_not()  # puts 0s where the prompt was, preserve last answer (padding tokens are EOS(2))\n",
    "\n",
    "            pos_labels[pos_labels == 0] = tokenizer.pad_token_id # replaces 0s with EOS(2)\n",
    "            neg_labels[neg_labels == tokenizer.pad_token_id] = -100 # change 2 to -100 so that loss calculations ignore prompt and padding\n",
    "            pos_labels[pos_labels == tokenizer.pad_token_id] = -100 # change 2 to -100 so that loss calculations ignore prompt and padding\n",
    "\n",
    "            # Run model for positive response\n",
    "            outputs_pos, loss_pos = model(batch['positive_input_ids'], pos_labels)  #  (1,1024) , (1,1024)\n",
    "            #positive input ids have all IDs including last answer and the padding has EOS 2\n",
    "            #pos_labels have everything set to -100 except the IDs of the last answer\n",
    "\n",
    "            # we don't use the negative loss for anything, that's why we didn't do a similar preparation here, but we use the\n",
    "            # output negative logits for the per token log probability calculations of the negative responses\n",
    "            outputs_neg, loss_neg = model(batch['negative_input_ids'], neg_labels)    \n",
    "\n",
    "            # CALCULATE PER TOKEN LOG PROBS, necessary to calculate ORPO ODDS ratio\n",
    "\n",
    "            # returns the average of The log probabilities for the positive samples (masking out prompt)\n",
    "            pos_prob = compute_logps(\n",
    "                prompt_attention_mask=batch['attention_mask'], \n",
    "                chosen_inputs=batch[\"positive_input_ids\"], \n",
    "                chosen_attention_mask=batch['positive_attention_mask'], \n",
    "                logits=outputs_pos\n",
    "            )\n",
    "            # returns the average of The log probabilities for the negative samples (masking out prompt)\n",
    "            neg_prob = compute_logps(\n",
    "                prompt_attention_mask=batch['attention_mask'], \n",
    "                chosen_inputs=batch[\"negative_input_ids\"], \n",
    "                chosen_attention_mask=batch['negative_attention_mask'], \n",
    "                logits=outputs_neg\n",
    "            )    \n",
    "\n",
    "            # CALCULATE ORPO ODDS RATIO\n",
    "            log_odds = (pos_prob - neg_prob) - (torch.log(1 - torch.exp(pos_prob)) - torch.log(1 - torch.exp(neg_prob)))\n",
    "            sig_ratio = F.sigmoid(log_odds) # constrain to be between 0 and 1\n",
    "            ratio = torch.log(sig_ratio) # apply the final log to the calculation\n",
    "\n",
    "            # Calculate the Final Total Loss, combination of standard Cross Entropy loss and the weighted Odds Ratio\n",
    "            loss = torch.mean(loss_pos - (alpha*ratio).mean()).to(dtype=dtype)\n",
    "            # notice that mean() is useful if batch size is larger than 1\n",
    "\n",
    "            # log info every few iterations\n",
    "            if i%log_iters == 0:\n",
    "\n",
    "                # Calculate average losses \n",
    "                loss_m, log_odds_m, ratio_m = calculate_loss()\n",
    "\n",
    "                print(f\"Epochs [{e}/{epochs}] Step: [{i}/{len(train_loader)}], train loss: {loss_m['train']:.4f}, val loss: {loss_m['val']:.4f}, Odds Ratio: {log_odds_m['train']:.4f}, val Odds Ratio: {log_odds_m['val']:.4f}\")\n",
    "                \n",
    "                if wandb_log:\n",
    "                    wandb.log({\n",
    "                        \"train_loss\": loss_m['train'],\n",
    "                        \"val_loss\": loss_m['val'],\n",
    "                        \"train_log_odds\": log_odds_m['train'],\n",
    "                        \"val_log_odds\": log_odds_m['val'],\n",
    "                        \"train_ratio\": (alpha*ratio_m['train']),\n",
    "                        \"val_ratio\": (alpha*ratio_m['val']),\n",
    "                        #\"pos_prob\": pos_prob.mean().item(),\n",
    "                        #\"neg_prob\": neg_prob.mean().item(),                        \n",
    "                        #\"lr\": scheduler.get_last_lr()[0],\n",
    "                    }, \n",
    "                    step = (e*len(train_loader) + i))\n",
    "\n",
    "                if torch.isnan(loss):\n",
    "                    if wandb_log:   \n",
    "                        wandb.finish()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    sys.exit()\n",
    "\n",
    "            loss.backward() # Calculate gradients\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip) # Clip gradients\n",
    "            optimizer.step() # Update model parameters\n",
    "            scheduler.step() # Update learning rate\n",
    "\n",
    "        # At the end of each epoch, save a checkpoint\n",
    "        sd = model.state_dict()\n",
    "        sd['config'] = config\n",
    "        torch.save(sd, os.path.join(checkpoint_dir, f'{project_name}_{e+1}.pt'))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Cleaning up...\")\n",
    "\n",
    "finally:\n",
    "    # Release GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory released.\")\n",
    "\n",
    "if wandb_log:   \n",
    "    wandb.finish()\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d92ddf-2782-4ff5-a1b5-508639b408c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
